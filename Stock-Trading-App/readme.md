# ğŸ“ˆ Polygon.io Tickers Exporter

A concise, productionâ€‘minded Python utility that pulls **reference tickers** from the Polygon.io v3 API, follows **cursorâ€‘based pagination** (`next_url`), and writes the unified payload to a clean **CSV**. The script is environmentâ€‘driven, resilient to sparse fields, and ready to slot into a data pipeline.

---

## ğŸš€ What This Does

* Calls `GET /v3/reference/tickers` with a configurable `limit` (default 1000).
* Iterates `next_url` until exhaustion, consolidating **all pages**.
* Infers the CSV schema from the first item returned, and safely handles missing keys for subsequent records.
* Emits `tickers.csv` with a stable header and one row per ticker document.

---

## ğŸ§© How It Works (from `script.py`)

* Loads `POLYGON_API_KEY` from `.env` using `pythonâ€‘dotenv`.
* Issues the first request: `https://api.polygon.io/v3/reference/tickers?limit=1000&apiKey=...`.
* Appends results, then follows `data["next_url"]` (when present), reâ€‘issuing requests until there are no more pages.
* Uses the **first** ticker dict as the header template; writes all rows via `csv.DictWriter`, backfilling missing keys with empty strings.

> Design choice: inferring the schema from the first element ensures a deterministic header while avoiding an O(n) union across all keys. If you need a full key union, see the *Extensions* section.

---

## ğŸ“‚ Project Layout

```
.
â”œâ”€â”€ script.py          # Main: fetch, paginate, write CSV
â”œâ”€â”€ requirements.txt   # requests, openai (optional), dotenv
â””â”€â”€ .env               # POLYGON_API_KEY
```

---

## âš™ï¸ Setup

### 1) Install Dependencies

```bash
pip install -r requirements.txt
```

### 2) Configure Environment

Create `.env` with your key:

```env
POLYGON_API_KEY="your_polygon_api_key_here"
```

### 3) Run

```bash
python script.py
```

Output example:

```
requesting next page https://api.polygon.io/v3/reference/tickers?cursor=...
requesting next page https://api.polygon.io/v3/reference/tickers?cursor=...
Wrote 9,893 rows to tickers.csv
```

---

## ğŸ§ª Notes & Assumptions

* **Pagination:** Polygon uses cursorâ€‘based pagination; the script follows `next_url` until absent.
* **Schema Stability:** The first record defines `fieldnames`. New/rare fields appearing later will be dropped unless you adopt the â€œunion of keysâ€ strategy below.
* **Encoding:** CSV is written with `encoding="utf-8"` and safe missingâ€‘key handling.
* **Rate Limits:** If you hit 429s, introduce `time.sleep()` between pages or request higher rate limits.

---

## ğŸ›¡ï¸ Error Handling & Hardening Ideas

* Add HTTP status checks (`response.raise_for_status()`), retry/backoff (e.g., `tenacity`), and defensive guards for `data.get("results", [])`.
* Validate that `tickers` is nonâ€‘empty before deriving `fieldnames`; fall back to a minimal header when empty.
* Log metadata (page count, total rows, runtime) for observability.

---

## ğŸ”§ Extensions (dropâ€‘in enhancements)

* **Union Schema:** Build a union of keys across all pages, then write header â†’ ensures no field loss.
* **CLI Flags:** `--market`, `--type`, `--active`, `--limit`, `--dest` for flexible usage.
* **Pandas Export:** Convert to DataFrame â†’ `to_parquet()`/`to_csv(index=False)` for analytics.
* **Incremental Loads:** Track last cursor or `updated` watermark; persist to a DB (SQLite/Postgres/Snowflake).
* **Validation:** Add lightweight Pydantic model to validate each ticker payload.

---

## âœ… Requirements

See `requirements.txt`:

* `requests`
* `dotenv` (aka `python-dotenv`)
* `openai` (not used by the script; keep if you plan to enrich with LLMâ€‘based tagging)

Python 3.10+ recommended.

---

## ğŸ“„ License

MIT (or your preferred). Add a `LICENSE` file if openâ€‘sourcing.

---

## ğŸ™Œ Attribution

Polygon.io Reference Data API â€” thanks for a clean cursorâ€‘based model that scales well for exports.
